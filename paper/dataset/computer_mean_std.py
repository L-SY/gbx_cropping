import torch
import torchvision.transforms as transforms
from PIL import Image
import os
import glob
from tqdm import tqdm
import numpy as np

def calculate_dataset_statistics(dataset_path, image_size=224, use_grayscale=False, batch_size=32):
    """
    è®¡ç®—æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®

    Args:
        dataset_path: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
        image_size: resizeåçš„å›¾ç‰‡å¤§å°
        use_grayscale: æ˜¯å¦è½¬ä¸ºç°åº¦å›¾
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
    """

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.webp']

    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = []
    for ext in image_extensions:
        image_files.extend(glob.glob(os.path.join(dataset_path, '**', ext), recursive=True))
        image_files.extend(glob.glob(os.path.join(dataset_path, '**', ext.upper()), recursive=True))

    if len(image_files) == 0:
        raise ValueError(f"No images found in {dataset_path}")

    print(f"ğŸ” Found {len(image_files)} images in {dataset_path}")

    # è®¾ç½®transform
    if use_grayscale:
        transform = transforms.Compose([
            transforms.Grayscale(num_output_channels=3),
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor()
        ])
    else:
        transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor()
        ])

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    channel_sum = torch.zeros(3)
    channel_squared_sum = torch.zeros(3)
    total_pixels = 0

    print("ğŸ“Š Calculating statistics...")

    # åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
    for i in tqdm(range(0, len(image_files), batch_size), desc="Processing batches"):
        batch_files = image_files[i:i + batch_size]
        batch_tensors = []

        # åŠ è½½å½“å‰æ‰¹æ¬¡çš„å›¾ç‰‡
        for img_path in batch_files:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img)
                batch_tensors.append(img_tensor)
            except Exception as e:
                print(f"âš ï¸  Warning: Could not load {img_path}: {e}")
                continue

        if batch_tensors:
            # å°†æ‰¹æ¬¡è½¬ä¸ºtensor
            batch = torch.stack(batch_tensors)  # Shape: (batch_size, 3, H, W)

            # ç´¯ç§¯ç»Ÿè®¡
            batch_sum = batch.sum(dim=[0, 2, 3])  # å¯¹batch, height, widthæ±‚å’Œ
            batch_squared_sum = (batch ** 2).sum(dim=[0, 2, 3])
            batch_pixels = batch.shape[0] * batch.shape[2] * batch.shape[3]

            channel_sum += batch_sum
            channel_squared_sum += batch_squared_sum
            total_pixels += batch_pixels

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡é‡
    mean = channel_sum / total_pixels
    var = (channel_squared_sum / total_pixels) - (mean ** 2)
    std = torch.sqrt(var)

    return mean, std, len(image_files)

def display_statistics(mean, std, num_images, dataset_path):
    """
    æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    """
    print("\n" + "="*60)
    print(f"ğŸ“ˆ Dataset Statistics for {dataset_path}")
    print("="*60)

    print(f"ğŸ“ Total images processed: {num_images}")
    print(f"ğŸ¨ Channel-wise statistics:")

    channels = ['Red  ', 'Green', 'Blue ']
    for i, channel in enumerate(channels):
        print(f"   {channel}: mean = {mean[i]:.6f}, std = {std[i]:.6f}")

    print(f"\nğŸ”§ Use these values in your transform:")
    print(f"transforms.Normalize(")
    print(f"    mean=[{mean[0]:.6f}, {mean[1]:.6f}, {mean[2]:.6f}],")
    print(f"    std=[{std[0]:.6f}, {std[1]:.6f}, {std[2]:.6f}]")
    print(f")")

    # ä¸ImageNetå¯¹æ¯”
    imagenet_mean = torch.tensor([0.485, 0.456, 0.406])
    imagenet_std = torch.tensor([0.229, 0.224, 0.225])

    print(f"\nğŸ“Š Comparison with ImageNet:")
    print(f"                Your Dataset    ImageNet        Difference")
    print(f"   Red   mean:  {mean[0]:.6f}        0.485000        {abs(mean[0]-imagenet_mean[0]):.6f}")
    print(f"   Green mean:  {mean[1]:.6f}        0.456000        {abs(mean[1]-imagenet_mean[1]):.6f}")
    print(f"   Blue  mean:  {mean[2]:.6f}        0.406000        {abs(mean[2]-imagenet_mean[2]):.6f}")
    print(f"   Red   std:   {std[0]:.6f}        0.229000        {abs(std[0]-imagenet_std[0]):.6f}")
    print(f"   Green std:   {std[1]:.6f}        0.224000        {abs(std[1]-imagenet_std[1]):.6f}")
    print(f"   Blue  std:   {std[2]:.6f}        0.225000        {abs(std[2]-imagenet_std[2]):.6f}")

    # ç»™å‡ºå»ºè®®
    mean_diff = torch.mean(torch.abs(mean - imagenet_mean)).item()
    std_diff = torch.mean(torch.abs(std - imagenet_std)).item()

    print(f"\nğŸ’¡ Recommendations:")
    if mean_diff > 0.1 or std_diff > 0.05:
        print(f"   âœ… Use your custom statistics (significant difference from ImageNet)")
        print(f"   âŒ Avoid ImageNet normalization for your dataset")
    else:
        print(f"   âœ… Your data is similar to ImageNet, both normalizations should work")
        print(f"   âœ… ImageNet normalization is fine to use")

def compare_normalizations(dataset_path, custom_mean, custom_std, sample_size=5):
    """
    å¯¹æ¯”ä¸åŒæ ‡å‡†åŒ–æ–¹æ³•çš„æ•ˆæœ
    """
    print(f"\n" + "="*60)
    print(f"ğŸ”¬ Normalization Comparison on Sample Images")
    print("="*60)

    # è·å–å‡ å¼ æ ·æœ¬å›¾ç‰‡
    image_files = glob.glob(os.path.join(dataset_path, "*.jpg")) + \
                  glob.glob(os.path.join(dataset_path, "*.png"))

    if len(image_files) == 0:
        print("No sample images found for comparison")
        return

    sample_files = image_files[:min(sample_size, len(image_files))]

    # ä¸åŒçš„æ ‡å‡†åŒ–æ–¹æ³•
    transforms_dict = {
        "No Normalization": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ]),
        "ImageNet Norm": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        "Custom Norm": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=custom_mean.tolist(), std=custom_std.tolist())
        ]),
        "Simple Norm": transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    }

    print(f"ğŸ“Š Processing {len(sample_files)} sample images...")

    for method_name, transform in transforms_dict.items():
        all_tensors = []

        for img_path in sample_files:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img)
                all_tensors.append(img_tensor)
            except Exception as e:
                continue

        if all_tensors:
            batch = torch.stack(all_tensors)
            mean_val = batch.mean().item()
            std_val = batch.std().item()
            min_val = batch.min().item()
            max_val = batch.max().item()

            print(f"\n{method_name:15}: range=[{min_val:7.3f}, {max_val:7.3f}], "
                  f"mean={mean_val:7.3f}, std={std_val:6.3f}")

def save_statistics(mean, std, num_images, save_path="./dataset_stats.txt"):
    """
    ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶
    """
    with open(save_path, 'w') as f:
        f.write(f"Dataset Statistics\n")
        f.write(f"==================\n")
        f.write(f"Total images: {num_images}\n")
        f.write(f"Mean: [{mean[0]:.6f}, {mean[1]:.6f}, {mean[2]:.6f}]\n")
        f.write(f"Std:  [{std[0]:.6f}, {std[1]:.6f}, {std[2]:.6f}]\n\n")
        f.write(f"PyTorch Transform Code:\n")
        f.write(f"transforms.Normalize(\n")
        f.write(f"    mean=[{mean[0]:.6f}, {mean[1]:.6f}, {mean[2]:.6f}],\n")
        f.write(f"    std=[{std[0]:.6f}, {std[1]:.6f}, {std[2]:.6f}]\n")
        f.write(f")\n")

    print(f"ğŸ’¾ Statistics saved to {save_path}")

# ä¸»å‡½æ•°
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    dataset_path = "/home/siyang_liu/gbx_cropping_ws/paper/images/augmented_dataset"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„
    image_size = 224    # ä¿®æ”¹ä¸ºä½ éœ€è¦çš„å›¾ç‰‡å¤§å°
    use_grayscale = True  # æ˜¯å¦è½¬ä¸ºç°åº¦å›¾
    batch_size = 32     # æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœå†…å­˜ä¸å¤Ÿå¯ä»¥è°ƒå°

    print("ğŸš€ Dataset Statistics Calculator")
    print(f"ğŸ“ Dataset path: {dataset_path}")
    print(f"ğŸ–¼ï¸  Image size: {image_size}x{image_size}")
    print(f"ğŸ¨ Grayscale: {use_grayscale}")
    print(f"ğŸ“¦ Batch size: {batch_size}")

    try:
        # è®¡ç®—ç»Ÿè®¡é‡
        mean, std, num_images = calculate_dataset_statistics(
            dataset_path=dataset_path,
            image_size=image_size,
            use_grayscale=use_grayscale,
            batch_size=batch_size
        )

        # æ˜¾ç¤ºç»“æœ
        display_statistics(mean, std, num_images, dataset_path)

        # ä¿å­˜ç»“æœ
        save_statistics(mean, std, num_images)

        # å¯¹æ¯”ä¸åŒæ ‡å‡†åŒ–æ–¹æ³•
        compare_normalizations(dataset_path, mean, std)

        print(f"\nâœ… Analysis complete! Use your custom normalization for best results.")

    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ğŸ’¡ Tips:")
        print("   - Check if the dataset path exists")
        print("   - Make sure there are images in the folder")
        print("   - Try reducing batch_size if out of memory")